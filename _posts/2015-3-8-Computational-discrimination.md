---
layout: post
category : publications
tags : [predictive modeling, discrimination, bias, fairness]
title : Unintentional discrimination in data-driven decision making
---
{% include JB/setup %}

In the era of big data more and more decisions are being automated using predictive models, built on historical data. Automated CV screening in job applications, customer profiling based on shopping history, or credit scoring for mortgages are just a few of many examples.

Even if the computation process is fair and wellâ€“intentioned, such predictive models can exhibit discrimination towards groups of people based upon, e.g., age or ethnicity. The reason for this phenomenon is that most data mining methods are based upon assumptions that are not always satisfied in reality, namely, that the data is correct and represents the population well.

A predictive model may embrace discriminatory decision procedures due to biased  data sampling procedure, data being incomplete, or data labels being incorrect. 
Our [book chapter](https://sites.google.com/site/zliobaitefiles/R1_chapter_calders_zliobaite.pdf) describes several scenarios how this may happen, and discusses computational techniques how to cope with that. All the book is [here](http://link.springer.com/book/10.1007/978-3-642-30487-3).
