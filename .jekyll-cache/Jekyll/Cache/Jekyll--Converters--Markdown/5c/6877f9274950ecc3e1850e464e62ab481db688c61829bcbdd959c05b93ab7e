I"∫0<p>This is a follow up on the post on <a href="/evolution/2021/07/30/Human-evolution1.html">trends in human evolution research</a>, outlining how the analysis was done. This is not an elegant solution, I apologise. I wanted a quick prototype, thus I used a zoo of tools. I‚Äôm sure most of the steps could be done more elegantly and efficiently with a bit more exploration, but my solution worked for the purpose (of exploration).</p>

<p>Most of the analysis was done in <code class="language-plaintext highlighter-rouge">Python</code>, a bit in <code class="language-plaintext highlighter-rouge">R</code>. If you are in palaeontology you probably generally prefer <code class="language-plaintext highlighter-rouge">R</code>, but for preprocessing texts you‚Äôll find better tools in <code class="language-plaintext highlighter-rouge">Python</code>. But if you are determined, you‚Äôll probably ca do a similar analysis in in <code class="language-plaintext highlighter-rouge">R</code>, tools are becioming available. If I were to implement this in <code class="language-plaintext highlighter-rouge">R</code> I‚Äôd probably start <a href="https://www.tidytextmining.com/">here</a>.</p>

<h3 id="1-getting-the-texts">1. Getting the texts</h3>

<p>So let‚Äôs begin. If we want to text-mine research articles, first thing we need to do is to get the actual texts. A set of texts for analysis is called <em>corpus</em>. You probably want to stay away from extracting texts from pdfs, so you need to find excess to full texts. When you find access to full texts, basically there are two ways to get the texts. The first one is to crawl the published website, which means to write a program that would visit each article and copy its text as if a user would do it manually. This is generally undesired, sometimes forbidden, and anyway needs to be timed well not to overload the traffic. A much better way to use APIs that publishers provide for the purpose of getting texts for text mining. Largest commercial publishers would very likely have them, but smaller ones perhaps won‚Äôt. And, the challenge is that operating an API would be different in each case.</p>

<p>Luckily, the Journal of Human Evolution was available via <a href="https://www.elsevier.com/about/policies/text-and-data-mining">Elsevier‚Äôs API</a>. for starters one needed to get an <a href="https://dev.elsevier.com/">API key</a>, that‚Äôs a personal identification number that Elsevier is using to track who is doing what. For next steps one needs to save the personal key into <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/1_getting_texts/config.json">config.json</a>, this is a template without the actual key, paste your key to replace ‚Äò123mykey‚Äô.</p>

<p>Next, one needs to decide what texts one wants to get. In this particular case one can get anything that can be searched in <a href="https://www.scopus.com/home.uri?zone=header&amp;origin=">Scopus</a> database of Elsevier. For me it was easy, I simply queried all the articles that belong to the journal with ISSN = ‚Äò0047-2484‚Äô (The Journal of Human Evolution). One can do keywords, but I did not.</p>

<p>Using <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/1_getting_texts/run1_get_articles.py">this script</a> I got <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/1_getting_texts/articles_JHE.csv">this csv table</a> listing all the articles published in the Journal of Human Evolution.</p>

<p>Next, I retrieved the full texts of the articles listed in the csv table. I used pii as the unique identifier (one could equally have used DOI). I took PII from the csv table, which we retrieved a step earlier, column no. 15. This saved each article in folder <code class="language-plaintext highlighter-rouge">data</code>. I <a href="https://github.com/zliobaite/text_mining_human_evolution/tree/main/1_getting_texts/data">shared two full-text files</a>, as an example of what arrived.  It‚Äôs text, there are no figures. But the texts contain meta data and links, which we‚Äôll need to remove before proceeding to analyzing research content.</p>

<h3 id="2-preprocessing">2. Preprocessing</h3>

<p>The next step is to clean and standardize the texts (convert nouns to singular, verbs to present tense and alike). <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/2_preprocessing/run2_texts_to_data.py">This script</a> cleans the texts and saves them into a csv file together with meta data about the articles. Meta data includes DOIs, PIIs, titles, page ranges, authors and more. The script does the following. <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/2_preprocessing/corpus_JHE_demo.csv"><code class="language-plaintext highlighter-rouge">corpus_JHE_demo.csv</code></a> shows the processed data formal looks, the file only has three articles, not the whole corpus.</p>

<p>The script reads all files from the directory <code class="language-plaintext highlighter-rouge">data</code>. That is the directory where we saved the texts retrieved from Scopus. Each file is one article. Each article is read into one long string of words. The script removes punctuations, removes double white spaces and converts characters to lower case using <code class="language-plaintext highlighter-rouge">clean_text</code> function.</p>

<p>Next I used <a href="https://www.nltk.org/">Natural Language Toolkit (NLTK)</a> of Python for converting texts into data that can be used for predictive modeling. I pushed the texts through a tokenizer, that is converted a sequence of characters into a sequence of tokens (‚Äúfeatures‚Äù).  Sometimes tokens can be more than one word, for example, bigrams or n-grams combining multiple words. Most commonly each word is one token, that‚Äôs what I did here. I used <a href="https://www.nltk.org/api/nltk.tokenize.html"><code class="language-plaintext highlighter-rouge">TweetTokenizer</code></a>. Then I removed English stopwords (NLKT) package has a tool for that (<code class="language-plaintext highlighter-rouge">nltk.corpus.stopwords.words('english')</code>). Then I removed the remaining words shorter than 2 characters and longer than 20 characters, this helped to get rid of remaining artifacts due to web links present in the texts.</p>

<p>Then I lemmatized the words. Lemmatization groups together inflected words such that they can be analyzed as having the same meaning. For example, the word ‚Äúbetter‚Äù is changed to ‚Äúgood‚Äù, or the word ‚Äúchewing‚Äù is changed to ‚Äúchew‚Äù. I used <a href="https://www.nltk.org/howto/wordnet.html"><code class="language-plaintext highlighter-rouge">WordNetLemmatizer</code></a> from NLTK.</p>

<p>Finally, I put back the processed text into one string again. This step is redundant because when I start analyzing the text, I‚Äôll need to tokenize it again. But I did it because I wanted to have all preprocessed texts in one csv file called [<code class="language-plaintext highlighter-rouge">corpus_JHE.csv</code>], you‚Äôll see this name in the script. This file is over 100MB, so I zipped it. For a quick view I made a subsample file <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/2_preprocessing/corpus_JHE_demo.csv"><code class="language-plaintext highlighter-rouge">corpus_JHE_demo.csv</code></a> that has three rows corresponding to three articles. The full corpus had 4014 articles.</p>

<p>There was one more step in the sequence. I wanted to exclude editorial announcements and other short pieces from the analysis. For that purpose I only included an article in the corpus if it had more than one page. For this I (unelegantly) used the dash from the page number column. Since the originally retrieved set of metadata missed a few page numbers, I filled them manually, thence the input file in preprocessing was <code class="language-plaintext highlighter-rouge">articles_JHE_added_pages.csv</code> instead of <code class="language-plaintext highlighter-rouge">articles_JHE.csv</code>, which we got in step 1.</p>

<h2 id="3-topic-modeling-and-sentiment-analysis">3. Topic modeling and sentiment analysis</h2>

<p>The next step is <a href="https://en.wikipedia.org/wiki/Topic_model">topic modeling</a>. I used <a href="https://jupyter.org/">Jupyter Notebook</a>, which is an interactive environment for coding and allows to change modeling parameters, rerun parts of the pipeline and visualize the results. Jupyter is not specific to text mining, and not necessary for this analysis, I just used it for convenience. The code can be executed as is in <code class="language-plaintext highlighter-rouge">python</code>.</p>

<p><a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/3_topic_modeling/run3_analyze.ipynb">This notebook</a> that contains my script for the analysis, it‚Äôs commented upon. And the results are discussed in <a href="/evolution/2021/07/30/Human-evolution1.html">the previous post</a>. The notebook includes topic modeling and sentiment classification.</p>

<p>Three csv files are saved as a result of this run:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">topics_keywords.csv</code> has 10 rows, each row is one topic resulting from the topic modeling. In the columns are relative importance of words in each topic. The words are in the column headers. The higher the number, the more important is this word for this topic. One can extract, for example, 30 most important words for each topic, as shown in <a href="/evolution/2021/07/30/Human-evolution1.html">the previous post</a>. One can visualize topics as <a href="https://amueller.github.io/word_cloud/">wordclouds</a>, as also shown in <a href="/evolution/2021/07/30/Human-evolution1.html">the previous post</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">dominant_topics.cs</code> for each document (one document is one row) gives fractions of assignments to topics. The topics are numbered from 0 to 9 in the columns and the fractions sum up to 100%(=1). This means that a document can be assignment, for example, 80% to Topic#1, 11% to Topic#3 and 9% to Topic#8.</li>
  <li><code class="language-plaintext highlighter-rouge">sentiments.csv</code> gives sentiment classification for each document. Each row is one document. Columns: <code class="language-plaintext highlighter-rouge">neg</code> ‚Äì fraction of negative sentiment per document,
<code class="language-plaintext highlighter-rouge">neu</code> ‚Äì fraction of neutral sentiment per document,
<code class="language-plaintext highlighter-rouge">pos</code> - fraction of positive sentiment per document,
compound ‚Äì overall polarity score which I am not using in the analysis,
<code class="language-plaintext highlighter-rouge">eid</code> and <code class="language-plaintext highlighter-rouge">pii</code> ‚Äì unique identifiers of the articles.</li>
</ul>

<p>The very last step is visualization of topics and sentiments over time. For my convenience I implemented it in <code class="language-plaintext highlighter-rouge">R</code>, the script is <a href="https://github.com/zliobaite/text_mining_human_evolution/blob/main/3_topic_modeling/run4_visualize_sentiments.R">here</a>. The output files of that run can be used for visualization. I aggregated the results over 5 year time bins. They are stored in folder <code class="language-plaintext highlighter-rouge">sentiment_plots</code>.  The output files are:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">data_all_articles.csv</code> ‚Äì meta data on all articles together with their topic assignments and sentiment classification.</li>
  <li><code class="language-plaintext highlighter-rouge">year_results.csv</code> ‚Äì fraction of topics within each time period.</li>
  <li><code class="language-plaintext highlighter-rouge">year_sentiment_pos.csv</code> ‚Äì the prevalence of positive sentiments over time within each topic.</li>
  <li><code class="language-plaintext highlighter-rouge">year_sentiment_neg.csv</code> ‚Äì the prevalence of negative sentiments over time within each topic.</li>
  <li><code class="language-plaintext highlighter-rouge">year_sentiment_neu.csv</code> ‚Äì the prevalence of neutral sentiments over time within each topic.</li>
</ul>

<p>The code mentioned in this post is stored on <a href="https://github.com/zliobaite/text_mining_human_evolution">GitHub</a>.</p>
:ET